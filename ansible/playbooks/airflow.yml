---
# Apache Airflow deployment
- name: Deploy Apache Airflow
  hosts: app_servers
  become: yes
  gather_facts: yes

  tasks:
    - name: Create Airflow directory structure
      file:
        path: "{{ item }}"
        state: directory
        mode: '0755'
      loop:
        - /opt/docker/airflow
        - /opt/docker/airflow/dags
        - /opt/docker/airflow/logs
        - /opt/docker/airflow/plugins
        - /opt/docker/airflow/config
        - /opt/docker/airflow/scripts

    - name: Generate Airflow Fernet key
      shell: python3 -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
      register: fernet_key_result
      when: airflow_fernet_key == ""

    - name: Set Airflow Fernet key
      set_fact:
        airflow_fernet_key: "{{ fernet_key_result.stdout }}"
      when: airflow_fernet_key == ""

    - name: Create PostgreSQL configuration for Airflow
      copy:
        content: |
          version: '3.8'
          
          services:
            postgres:
              image: postgres:13
              container_name: airflow-postgres
              environment:
                POSTGRES_USER: {{ postgres_user }}
                POSTGRES_PASSWORD: {{ postgres_password }}
                POSTGRES_DB: {{ postgres_db }}
                POSTGRES_INITDB_ARGS: "--encoding=UTF-8"
              volumes:
                - postgres_data:/var/lib/postgresql/data
              ports:
                - "5432:5432"
              networks:
                - {{ app_network }}
              deploy:
                resources:
                  limits:
                    memory: 1G
                  reservations:
                    memory: 512M
          
          volumes:
            postgres_data:
              external: true
          
          networks:
            {{ app_network }}:
              external: true
        dest: /opt/docker/airflow/postgres-compose.yml
        mode: '0644'

    - name: Create Airflow configuration
      copy:
        content: |
          [core]
          dags_folder = /opt/airflow/dags
          hostname_callable = airflow.utils.net.get_host_ip_address
          default_timezone = utc
          executor = {{ airflow_executor }}
          parallelism = 32
          max_active_tasks_per_dag = 16
          dags_are_paused_at_creation = True
          max_active_runs_per_dag = 16
          load_examples = False
          plugins_folder = /opt/airflow/plugins
          secret_key = {{ airflow_fernet_key }}
          
          [database]
          sql_alchemy_conn = postgresql+psycopg2://{{ postgres_user }}:{{ postgres_password }}@postgres:5432/{{ postgres_db }}
          sql_engine_encoding = utf-8
          sql_alchemy_pool_enabled = True
          sql_alchemy_pool_size = 5
          sql_alchemy_max_overflow = 10
          sql_alchemy_pool_recycle = 3600
          
          [logging]
          logging_level = INFO
          fab_logging_level = WARN
          logging_config_class =
          colored_console_log = False
          colored_log_format = [%%(blue)s%%(asctime)s%%(reset)s] {%%(blue)s%%(filename)s:%%(reset)s%%(lineno)d} %%(log_color)s%%(levelname)s%%(reset)s - %%(log_color)s%%(message)s%%(reset)s
          colored_formatter_class = airflow.utils.log.colored_log.CustomTTYColoredFormatter
          log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
          simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s
          dag_processor_manager_log_location = /opt/airflow/logs/dag_processor_manager/dag_processor_manager.log
          child_process_log_directory = /opt/airflow/logs/scheduler
          base_log_folder = /opt/airflow/logs
          processor_log_folder = /opt/airflow/logs/scheduler
          
          [metrics]
          statsd_on = False
          statsd_host = localhost
          statsd_port = 8125
          statsd_prefix = airflow
          
          [webserver]
          base_url = http://{{ ansible_default_ipv4.address }}:{{ airflow_web_port }}
          default_ui_timezone = UTC
          web_server_host = 0.0.0.0
          web_server_port = {{ airflow_web_port }}
          web_server_worker_timeout = 120
          worker_refresh_batch_size = 1
          worker_refresh_interval = 6000
          secret_key = {{ airflow_fernet_key }}
          workers = 4
          worker_class = sync
          access_logfile = -
          error_logfile = -
          expose_config = False
          authenticate = False
          filter_by_owner = False
          enable_proxy_fix = True
          
          [email]
          email_backend = airflow.utils.email.send_email_smtp
          email_conn_id = smtp_default
          default_email_on_retry = True
          default_email_on_failure = True
          
          [smtp]
          smtp_host = localhost
          smtp_starttls = True
          smtp_ssl = False
          smtp_port = 587
          smtp_mail_from = airflow@{{ ansible_hostname }}
          
          [celery]
          celery_app_name = airflow.executors.celery_executor
          worker_concurrency = 16
          
          [celery_kubernetes_executor]
          kubernetes_queue = kubernetes
          
          [scheduler]
          job_heartbeat_sec = 5
          scheduler_heartbeat_sec = 5
          run_duration = -1
          min_file_process_interval = 0
          dag_dir_list_interval = 300
          print_stats_interval = 30
          child_process_log_directory = /opt/airflow/logs/scheduler
          scheduler_zombie_task_threshold = 300
          catchup_by_default = False
          max_threads = 2
          parsing_processes = 2
          
          [kerberos]
          ccache = /tmp/airflow_krb5_ccache
          principal = airflow
          reinit_frequency = 3600
          kinit_path = kinit
          keytab = airflow.keytab
          
          [github_enterprise]
          api_rev = v3
          
          [admin]
          hide_sensitive_variable_fields = True
          
          [elasticsearch]
          host = elasticsearch
          log_id_template = {dag_id}-{task_id}-{execution_date}-{try_number}
          end_of_log_mark = end_of_log
          frontend = elasticsearch
          write_stdout = False
          json_format = False
          json_fields = asctime, filename, lineno, levelname, message
          
          [elasticsearch_configs]
          max_retries = 3
          timeout = 30
          retry_timeout = True
        dest: /opt/docker/airflow/config/airflow.cfg
        mode: '0644'

    - name: Create Airflow Docker Compose file
      copy:
        content: |
          version: '3.8'
          
          x-airflow-common:
            &airflow-common
            image: apache/airflow:{{ airflow_version }}
            environment: &airflow-common-env
              AIRFLOW__CORE__EXECUTOR: {{ airflow_executor }}
              AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://{{ postgres_user }}:{{ postgres_password }}@postgres:5432/{{ postgres_db }}
              AIRFLOW__CORE__FERNET_KEY: '{{ airflow_fernet_key }}'
              AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
              AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
              AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
              AIRFLOW__WEBSERVER__BASE_URL: 'http://{{ ansible_default_ipv4.address }}:{{ airflow_web_port }}'
              AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
              AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: 'true'
              AIRFLOW__LOGGING__LOGGING_LEVEL: 'INFO'
              AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: 'false'
              AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: '300'
              AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: '1'
              _PIP_ADDITIONAL_REQUIREMENTS: 'psycopg2-binary elasticsearch'
            volumes:
              - /opt/docker/airflow/dags:/opt/airflow/dags
              - /opt/docker/airflow/logs:/opt/airflow/logs
              - /opt/docker/airflow/plugins:/opt/airflow/plugins
              - /opt/docker/airflow/config/airflow.cfg:/opt/airflow/airflow.cfg:ro
            user: "${AIRFLOW_UID:-50000}:0"
            depends_on: &airflow-common-depends-on
              postgres:
                condition: service_healthy
          
          services:
            postgres:
              image: postgres:13
              container_name: airflow-postgres
              environment:
                POSTGRES_USER: {{ postgres_user }}
                POSTGRES_PASSWORD: {{ postgres_password }}
                POSTGRES_DB: {{ postgres_db }}
              volumes:
                - postgres_data:/var/lib/postgresql/data
              ports:
                - "5432:5432"
              healthcheck:
                test: ["CMD", "pg_isready", "-U", "{{ postgres_user }}"]
                interval: 5s
                retries: 5
              restart: always
              networks:
                - {{ app_network }}
              deploy:
                resources:
                  limits:
                    memory: 1G
                  reservations:
                    memory: 512M
          
            airflow-webserver:
              <<: *airflow-common
              container_name: airflow-webserver
              command: webserver
              ports:
                - "{{ airflow_web_port }}:8080"
              healthcheck:
                test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
                interval: 10s
                timeout: 10s
                retries: 5
              restart: always
              depends_on:
                <<: *airflow-common-depends-on
                airflow-init:
                  condition: service_completed_successfully
              networks:
                - {{ app_network }}
                - {{ logging_network }}
              logging:
                driver: fluentd
                options:
                  fluentd-address: localhost:24224
                  tag: airflow.webserver
              deploy:
                resources:
                  limits:
                    memory: 1G
                  reservations:
                    memory: 512M
                placement:
                  constraints: [node.role == manager]
          
            airflow-scheduler:
              <<: *airflow-common
              container_name: airflow-scheduler
              command: scheduler
              healthcheck:
                test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
                interval: 10s
                timeout: 10s
                retries: 5
              restart: always
              depends_on:
                <<: *airflow-common-depends-on
                airflow-init:
                  condition: service_completed_successfully
              networks:
                - {{ app_network }}
                - {{ logging_network }}
              logging:
                driver: fluentd
                options:
                  fluentd-address: localhost:24224
                  tag: airflow.scheduler
              deploy:
                resources:
                  limits:
                    memory: 2G
                  reservations:
                    memory: 1G
          
            airflow-worker:
              <<: *airflow-common
              container_name: airflow-worker
              command: celery worker
              healthcheck:
                test:
                  - "CMD-SHELL"
                  - 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
                interval: 10s
                timeout: 10s
                retries: 5
              environment:
                <<: *airflow-common-env
                DUMB_INIT_SETSID: "0"
              restart: always
              depends_on:
                <<: *airflow-common-depends-on
                airflow-init:
                  condition: service_completed_successfully
              networks:
                - {{ app_network }}
                - {{ logging_network }}
              logging:
                driver: fluentd
                options:
                  fluentd-address: localhost:24224
                  tag: airflow.worker
              deploy:
                resources:
                  limits:
                    memory: 2G
                  reservations:
                    memory: 1G
          
            airflow-triggerer:
              <<: *airflow-common
              container_name: airflow-triggerer
              command: triggerer
              healthcheck:
                test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
                interval: 10s
                timeout: 10s
                retries: 5
              restart: always
              depends_on:
                <<: *airflow-common-depends-on
                airflow-init:
                  condition: service_completed_successfully
              networks:
                - {{ app_network }}
                - {{ logging_network }}
              logging:
                driver: fluentd
                options:
                  fluentd-address: localhost:24224
                  tag: airflow.triggerer
              deploy:
                resources:
                  limits:
                    memory: 512M
                  reservations:
                    memory: 256M
          
            airflow-init:
              <<: *airflow-common
              container_name: airflow-init
              entrypoint: /bin/bash
              command:
                - -c
                - |
                  function ver() {
                    printf "%04d%04d%04d%04d" $${1//./ }
                  }
                  airflow_version=$$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO && airflow version)
                  airflow_version_comparable=$$(ver $${airflow_version})
                  min_airflow_version=2.2.0
                  min_airflow_version_comparable=$$(ver $${min_airflow_version})
                  if (( airflow_version_comparable < min_airflow_version_comparable )); then
                    echo
                    echo -e "\033[1;31mERROR!!!: Too old Airflow version $${airflow_version}!\e[0m"
                    echo "The minimum Airflow version supported: $${min_airflow_version}. Only use this or higher!"
                    echo
                    exit 1
                  fi
                  if [[ -z "$${AIRFLOW_UID}" ]]; then
                    echo
                    echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
                    echo "If you are on Linux, you SHOULD follow the instructions below to set "
                    echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
                    echo "For other operating systems you can get rid of the warning with manually created .env file:"
                    echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#setting-the-right-airflow-user"
                    echo
                  fi
                  one_meg=1048576
                  mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
                  cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
                  disk_available=$$(df / | tail -1 | awk '{print $$4}')
                  warning_resources="false"
                  if (( mem_available < 4000 )) ; then
                    echo
                    echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
                    echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
                    echo
                    warning_resources="true"
                  fi
                  if (( cpus_available < 2 )); then
                    echo
                    echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
                    echo "At least 2 CPUs recommended. You have $${cpus_available}"
                    echo
                    warning_resources="true"
                  fi
                  if (( disk_available < one_meg * 10 )); then
                    echo
                    echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
                    echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
                    echo
                    warning_resources="true"
                  fi
                  if [[ $${warning_resources} == "true" ]]; then
                    echo
                    echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
                    echo "Please follow the instructions to increase amount of resources available:"
                    echo "   https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#before-you-begin"
                    echo
                  fi
                  mkdir -p /sources/logs /sources/dags /sources/plugins
                  chown -R "$${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
                  exec /entrypoint airflow version
              environment:
                <<: *airflow-common-env
                _AIRFLOW_DB_UPGRADE: 'true'
                _AIRFLOW_WWW_USER_CREATE: 'true'
                _AIRFLOW_WWW_USER_USERNAME: admin
                _AIRFLOW_WWW_USER_PASSWORD: admin
              user: "0:0"
              volumes:
                - /opt/docker/airflow:/sources
              networks:
                - {{ app_network }}
          
            flower:
              <<: *airflow-common
              container_name: airflow-flower
              command: celery flower
              ports:
                - "5555:5555"
              healthcheck:
                test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
                interval: 10s
                timeout: 10s
                retries: 5
              restart: always
              depends_on:
                <<: *airflow-common-depends-on
                airflow-init:
                  condition: service_completed_successfully
              networks:
                - {{ app_network }}
              deploy:
                resources:
                  limits:
                    memory: 512M
                  reservations:
                    memory: 256M
          
          volumes:
            postgres_data:
              external: true
          
          networks:
            {{ app_network }}:
              external: true
            {{ logging_network }}:
              external: true
        dest: /opt/docker/airflow/docker-compose.yml
        mode: '0644'

    - name: Create example DAG
      copy:
        content: |
          from datetime import datetime, timedelta
          from airflow import DAG
          from airflow.operators.bash import BashOperator
          from airflow.operators.python import PythonOperator
          from airflow.operators.dummy import DummyOperator
          
          default_args = {
              'owner': 'devops-team',
              'depends_on_past': False,
              'start_date': datetime(2024, 1, 1),
              'email_on_failure': False,
              'email_on_retry': False,
              'retries': 1,
              'retry_delay': timedelta(minutes=5),
          }
          
          dag = DAG(
              'example_devops_pipeline',
              default_args=default_args,
              description='Example DevOps pipeline DAG',
              schedule_interval=timedelta(hours=1),
              catchup=False,
              tags=['example', 'devops'],
          )
          
          def log_system_info():
              import subprocess
              import logging
              
              # Get system information
              hostname = subprocess.check_output(['hostname']).decode().strip()
              uptime = subprocess.check_output(['uptime']).decode().strip()
              df_output = subprocess.check_output(['df', '-h']).decode()
              
              logging.info(f"Hostname: {hostname}")
              logging.info(f"Uptime: {uptime}")
              logging.info(f"Disk usage:\n{df_output}")
              
              return f"System check completed for {hostname}"
          
          # Define tasks
          start_task = DummyOperator(
              task_id='start',
              dag=dag,
          )
          
          system_check = PythonOperator(
              task_id='system_check',
              python_callable=log_system_info,
              dag=dag,
          )
          
          docker_status = BashOperator(
              task_id='docker_status',
              bash_command='docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"',
              dag=dag,
          )
          
          log_check = BashOperator(
              task_id='log_check',
              bash_command='curl -s http://localhost:9200/_cat/indices?v | head -10',
              dag=dag,
          )
          
          end_task = DummyOperator(
              task_id='end',
              dag=dag,
          )
          
          # Define task dependencies
          start_task >> [system_check, docker_status, log_check] >> end_task
        dest: /opt/docker/airflow/dags/example_devops_pipeline.py
        mode: '0644'

    - name: Create Airflow health check script
      copy:
        content: |
          #!/bin/bash
          # Airflow health check script
          
          AIRFLOW_URL="http://localhost:{{ airflow_web_port }}/health"
          
          # Check if Airflow webserver is responding
          HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" "$AIRFLOW_URL")
          
          if [ "$HTTP_STATUS" -eq "200" ]; then
              echo "$(date): Airflow is healthy - HTTP $HTTP_STATUS"
              exit 0
          else
              echo "$(date): Airflow health check failed - HTTP $HTTP_STATUS"
              exit 1
          fi
        dest: /opt/docker/airflow/scripts/health-check.sh
        mode: '0755'

    - name: Set proper ownership for Airflow directories
      file:
        path: "{{ item }}"
        owner: 50000
        group: 0
        recurse: yes
      loop:
        - /opt/docker/airflow/dags
        - /opt/docker/airflow/logs
        - /opt/docker/airflow/plugins

    - name: Deploy Airflow stack
      command: docker stack deploy -c /opt/docker/airflow/docker-compose.yml airflow
      args:
        chdir: /opt/docker/airflow
      environment:
        AIRFLOW_UID: 50000

    - name: Wait for Airflow to be ready
      uri:
        url: "http://{{ ansible_default_ipv4.address }}:{{ airflow_web_port }}/health"
        method: GET
        status_code: 200
      register: result
      until: result.status == 200
      retries: 30
      delay: 15

    - name: Configure firewall for Airflow
      ufw:
        rule: allow
        port: "{{ item }}"
        proto: tcp
      loop:
        - "{{ airflow_web_port }}"
        - "5432"  # PostgreSQL
        - "5555"  # Flower
      when: firewall_enabled | default(true)

    - name: Display Airflow information
      debug:
        msg: |
          Apache Airflow deployed successfully!
          
          Access Information:
          - Airflow UI: http://{{ ansible_default_ipv4.address }}:{{ airflow_web_port }}/
          - Via Nginx proxy: http://{{ ansible_default_ipv4.address }}/airflow/
          - Flower (Celery monitoring): http://{{ ansible_default_ipv4.address }}:5555/
          
          Default Credentials:
          - Username: admin
          - Password: admin
          
          Services:
          - Airflow Webserver: {{ ansible_default_ipv4.address }}:{{ airflow_web_port }}
          - PostgreSQL: {{ ansible_default_ipv4.address }}:5432
          - Flower: {{ ansible_default_ipv4.address }}:5555
          
          Features:
          - {{ airflow_executor }} executor
          - PostgreSQL backend database
          - Celery worker for distributed task execution
          - Flower for Celery monitoring
          - Logging to ELK stack via Fluentd
          - Health monitoring
          - Example DevOps pipeline DAG included
          
          Configuration:
          - DAGs directory: /opt/docker/airflow/dags
          - Logs directory: /opt/docker/airflow/logs
          - Plugins directory: /opt/docker/airflow/plugins
          - Config file: /opt/docker/airflow/config/airflow.cfg
          
          Database:
          - Type: PostgreSQL
          - Database: {{ postgres_db }}
          - User: {{ postgres_user }}
          
          Security:
          - Fernet key generated and configured
          - Basic authentication enabled
          - Proxy-friendly configuration
          
          Example DAG 'example_devops_pipeline' is included and can be enabled in the UI.